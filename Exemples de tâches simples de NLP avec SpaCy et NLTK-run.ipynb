{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook accompagne le post : \"Handson de quelques tâches courantes en NLP\" (ou sa version anglaise \"Simple NLP tasks tutorial\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement des modèles SpaCy via le notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Downloading en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
      "\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz (52.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 52.2MB 12.0MB/s ta 0:00:01   18% |█████▉                          | 9.5MB 10.9MB/s eta 0:00:04    20% |██████▋                         | 10.7MB 10.9MB/s eta 0:00:04    41% |█████████████▍                  | 21.8MB 12.3MB/s eta 0:00:03    45% |██████████████▌                 | 23.6MB 12.3MB/s eta 0:00:03    65% |████████████████████▉           | 34.0MB 12.2MB/s eta 0:00:02    68% |██████████████████████          | 35.9MB 10.7MB/s eta 0:00:02    74% |████████████████████████        | 39.0MB 12.3MB/s eta 0:00:02    85% |███████████████████████████▎    | 44.5MB 12.2MB/s eta 0:00:01    88% |████████████████████████████▍   | 46.4MB 12.5MB/s eta 0:00:01    92% |█████████████████████████████▋  | 48.2MB 10.7MB/s eta 0:00:01    97% |███████████████████████████████ | 50.7MB 12.3MB/s eta 0:00:01    99% |███████████████████████████████▉| 51.9MB 12.3MB/s eta 0:00:01\n",
      "\u001b[?25h  Requirement already satisfied (use --upgrade to upgrade): en-core-web-sm==1.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages\n",
      "Requirement already satisfied: spacy<2.0.0,>=1.7.0 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: six in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: pip<10.0.0,>=9.0.0 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: thinc<6.6.0,>=6.5.0 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: regex<2017.12.1,>=2017.4.1 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: pathlib in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: murmurhash<0.27,>=0.26 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: ujson>=1.35 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: termcolor in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: wrapt in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: html5lib in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: wcwidth in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: webencodings in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "\n",
      "    /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/en_core_web_sm/en_core_web_sm-1.2.0\n",
      "    -->\n",
      "    /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en').\n",
      "\n",
      "\n",
      "    Downloading fr_depvec_web_lg-1.0.0/fr_depvec_web_lg-1.0.0.tar.gz\n",
      "\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/fr_depvec_web_lg-1.0.0/fr_depvec_web_lg-1.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_depvec_web_lg-1.0.0/fr_depvec_web_lg-1.0.0.tar.gz (1424.8MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K    22% |███████▏                        | 317.2MB 1.5MB/s eta 0:12:430 0% |                                | 1.3MB 11.5MB/s eta 0:02:04    0% |                                | 3.4MB 12.2MB/s eta 0:01:57    0% |▎                               | 10.0MB 12.2MB/s eta 0:01:56    0% |▎                               | 13.5MB 21.6MB/s eta 0:01:06    1% |▍                               | 17.6MB 12.1MB/s eta 0:01:57    1% |▌                               | 22.1MB 10.7MB/s eta 0:02:11    1% |▌                               | 24.5MB 10.7MB/s eta 0:02:11    2% |▉                               | 34.9MB 10.9MB/s eta 0:02:08    2% |▉                               | 37.4MB 10.9MB/s eta 0:02:07    2% |█                               | 41.1MB 10.9MB/s eta 0:02:08    3% |█                               | 43.6MB 10.9MB/s eta 0:02:08    3% |█                               | 49.1MB 12.3MB/s eta 0:01:52    3% |█▎                              | 54.6MB 10.9MB/s eta 0:02:06    3% |█▎                              | 55.9MB 10.9MB/s eta 0:02:06    4% |█▋                              | 70.6MB 10.9MB/s eta 0:02:04    5% |█▋                              | 71.8MB 10.8MB/s eta 0:02:06    5% |█▉                              | 81.1MB 10.7MB/s eta 0:02:06    5% |█▉                              | 83.5MB 10.9MB/s eta 0:02:04    6% |██                              | 86.0MB 11.9MB/s eta 0:01:53    6% |██                              | 87.8MB 12.2MB/s eta 0:01:50    6% |██                              | 92.1MB 12.3MB/s eta 0:01:49    6% |██▏                             | 96.5MB 10.9MB/s eta 0:02:03    7% |██▎                             | 101.4MB 12.3MB/s eta 0:01:48    7% |██▎                             | 102.6MB 12.2MB/s eta 0:01:49    7% |██▎                             | 103.2MB 10.8MB/s eta 0:02:03    7% |██▌                             | 112.9MB 10.8MB/s eta 0:02:02    7% |██▌                             | 113.5MB 10.7MB/s eta 0:02:03    8% |██▋                             | 114.2MB 12.3MB/s eta 0:01:47    8% |██▋                             | 116.6MB 11.0MB/s eta 0:01:59    8% |██▉                             | 125.9MB 12.4MB/s eta 0:01:46    9% |███                             | 130.2MB 12.2MB/s eta 0:01:46    9% |███                             | 130.8MB 12.2MB/s eta 0:01:46    9% |███                             | 132.1MB 10.9MB/s eta 0:01:59    9% |███                             | 136.4MB 10.8MB/s eta 0:02:00    9% |███                             | 137.0MB 12.2MB/s eta 0:01:46    9% |███▏                            | 139.5MB 12.5MB/s eta 0:01:43    9% |███▏                            | 140.1MB 11.0MB/s eta 0:01:58    9% |███▏                            | 140.7MB 10.7MB/s eta 0:02:00    10% |███▎                            | 145.0MB 12.1MB/s eta 0:01:46    10% |███▎                            | 145.7MB 10.8MB/s eta 0:01:59    10% |███▎                            | 147.5MB 10.8MB/s eta 0:01:59    10% |███▍                            | 148.8MB 12.2MB/s eta 0:01:45    10% |███▍                            | 150.0MB 10.9MB/s eta 0:01:58    10% |███▍                            | 151.8MB 12.4MB/s eta 0:01:43    10% |███▍                            | 152.4MB 12.2MB/s eta 0:01:44    10% |███▌                            | 156.2MB 12.4MB/s eta 0:01:43    11% |███▌                            | 158.1MB 12.6MB/s eta 0:01:41    12% |███▉                            | 171.6MB 12.2MB/s eta 0:01:43    12% |███▉                            | 172.9MB 10.9MB/s eta 0:01:56                         | 174.7MB 11.9MB/s eta 0:01:46    12% |████                            | 179.0MB 10.7MB/s eta 0:01:57    12% |████                            | 179.7MB 12.3MB/s eta 0:01:42    12% |████                            | 181.5MB 12.3MB/s eta 0:01:42    13% |████▏                           | 187.7MB 12.3MB/s eta 0:01:41    13% |████▎                           | 188.3MB 12.5MB/s eta 0:01:40    13% |████▎                           | 190.2MB 11.1MB/s eta 0:01:52    13% |████▍                           | 193.3MB 12.3MB/s eta 0:01:41    13% |████▍                           | 195.1MB 11.0MB/s eta 0:01:52    13% |████▍                           | 196.4MB 12.4MB/s eta 0:01:40    13% |████▍                           | 197.6MB 12.4MB/s eta 0:01:39    14% |████▌                           | 200.7MB 12.3MB/s eta 0:01:40    14% |████▌                           | 201.3MB 12.6MB/s eta 0:01:37    14% |████▌                           | 202.0MB 12.3MB/s eta 0:01:40    14% |████▋                           | 206.3MB 10.9MB/s eta 0:01:52    14% |████▋                           | 206.9MB 10.7MB/s eta 0:01:54    14% |████▋                           | 207.5MB 12.3MB/s eta 0:01:40    14% |████▊                           | 209.4MB 10.9MB/s eta 0:01:52    14% |████▊                           | 210.6MB 12.2MB/s eta 0:01:40    15% |████▉                           | 214.9MB 12.2MB/s eta 0:01:39    15% |████▉                           | 216.2MB 10.9MB/s eta 0:01:51    15% |█████                           | 218.6MB 12.3MB/s eta 0:01:39    15% |█████                           | 219.3MB 12.5MB/s eta 0:01:37    15% |█████                           | 222.4MB 12.3MB/s eta 0:01:38    15% |█████                           | 223.6MB 12.3MB/s eta 0:01:38    15% |█████                           | 227.3MB 12.5MB/s eta 0:01:37    16% |█████▏                          | 232.3MB 12.5MB/s eta 0:01:36    16% |█████▎                          | 234.2MB 12.4MB/s eta 0:01:37    16% |█████▎                          | 234.8MB 10.7MB/s eta 0:01:52    16% |█████▎                          | 235.4MB 10.9MB/s eta 0:01:49    16% |█████▍                          | 240.4MB 10.6MB/s eta 0:01:52    17% |█████▍                          | 242.2MB 12.5MB/s eta 0:01:35    17% |█████▌                          | 247.2MB 11.0MB/s eta 0:01:48    17% |█████▋                          | 249.1MB 10.9MB/s eta 0:01:48    17% |█████▊                          | 254.7MB 12.3MB/s eta 0:01:36    17% |█████▊                          | 255.3MB 12.3MB/s eta 0:01:35    18% |██████                          | 263.9MB 10.9MB/s eta 0:01:47    18% |██████                          | 268.3MB 12.3MB/s eta 0:01:35    18% |██████                          | 269.5MB 12.2MB/s eta 0:01:35    19% |██████▏                         | 272.7MB 10.8MB/s eta 0:01:47    19% |██████▏                         | 275.2MB 10.7MB/s eta 0:01:48    19% |██████▍                         | 284.6MB 12.2MB/s eta 0:01:34    20% |██████▍                         | 285.2MB 12.5MB/s eta 0:01:32    20% |██████▌                         | 288.3MB 12.3MB/s eta 0:01:33    20% |██████▌                         | 290.8MB 12.2MB/s eta 0:01:34    20% |██████▌                         | 291.4MB 10.9MB/s eta 0:01:44    20% |██████▋                         | 292.0MB 10.7MB/s eta 0:01:47    20% |██████▋                         | 292.6MB 12.4MB/s eta 0:01:32    20% |██████▋                         | 295.8MB 12.5MB/s eta 0:01:31    20% |██████▊                         | 297.6MB 12.3MB/s eta 0:01:32    21% |██████▊                         | 300.7MB 12.3MB/s eta 0:01:32    21% |██████▊                         | 301.3MB 12.3MB/s eta 0:01:32    21% |██████▉                         | 302.0MB 10.9MB/s eta 0:01:43    21% |██████▉                         | 304.5MB 12.2MB/s eta 0:01:32    21% |██████▉                         | 305.7MB 12.2MB/s eta 0:01:32\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/_vendor/requests/packages/urllib3/response.py\", line 232, in _error_catcher\n",
      "    yield\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/_vendor/requests/packages/urllib3/response.py\", line 314, in read\n",
      "    data = self._fp.read(amt)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/http/client.py\", line 448, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/http/client.py\", line 488, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/socket.py\", line 576, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/ssl.py\", line 937, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/ssl.py\", line 799, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/ssl.py\", line 583, in read\n",
      "    v = self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/commands/install.py\", line 324, in run\n",
      "    requirement_set.prepare_files(finder)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/req/req_set.py\", line 380, in prepare_files\n",
      "    ignore_dependencies=self.ignore_dependencies))\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/req/req_set.py\", line 620, in _prepare_file\n",
      "    session=self.session, hashes=hashes)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/download.py\", line 821, in unpack_url\n",
      "    hashes=hashes\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/download.py\", line 659, in unpack_http_url\n",
      "    hashes)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/download.py\", line 882, in _download_http_url\n",
      "    _download_url(resp, link, content_file, hashes)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/download.py\", line 605, in _download_url\n",
      "    consume(downloaded_chunks)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/utils/__init__.py\", line 852, in consume\n",
      "    deque(iterator, maxlen=0)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/download.py\", line 571, in written_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/utils/ui.py\", line 139, in iter\n",
      "    for x in it:\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/download.py\", line 560, in resp_read\n",
      "    decode_content=False):\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/_vendor/requests/packages/urllib3/response.py\", line 357, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/_vendor/requests/packages/urllib3/response.py\", line 324, in read\n",
      "    flush_decoder = True\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/contextlib.py\", line 77, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/pip/_vendor/requests/packages/urllib3/response.py\", line 237, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n",
      "pip._vendor.requests.packages.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='github-production-release-asset-2e65be.s3.amazonaws.com', port=443): Read timed out.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[93m    Linking successful\u001b[0m\r\n",
      "\r\n",
      "    /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/fr_depvec_web_lg/fr_depvec_web_lg-1.0.0\r\n",
      "    -->\r\n",
      "    /home/pauline/anaconda3/envs/nltk_spacy_test_env/lib/python3.5/site-packages/spacy/data/fr\r\n",
      "\r\n",
      "    You can now load the model via spacy.load('fr').\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en # modèle anglais\n",
    "!python -m spacy download fr # modèle français"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des librairies SpaCy et NLTK et création des \"language processing pipelines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load(\"fr\")\n",
    "nlp_en = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation de texte en phrases\n",
    "\n",
    "Il s'agit de repérer le début et la fin de chaque phrase d'un texte pour pouvoir la segmenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du texte à segmenter\n",
    "text_fr = \"Ceci est 1 première phrase. Puis j'en écris une seconde. pour finir en voilà une troisième sans mettre de majuscule\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage du texte par le pipeline\n",
    "doc_fr = nlp_fr(text_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceci est 1 première phrase. Puis j'en\n",
      "écris une seconde.\n",
      "pour finir en voilà une troisième sans mettre de majuscule\n"
     ]
    }
   ],
   "source": [
    "# Segmentation du texte et affichage du résulat\n",
    "for sent in doc_fr.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction permettant la segmentation en phrases\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceci est 1 première phrase.\n",
      "Puis j'en écris une seconde.\n",
      "pour finir en voilà une troisième sans mettre de majuscule\n"
     ]
    }
   ],
   "source": [
    "# Segmentation du texte et affichage du résultat\n",
    "sentences = sent_tokenize(text_fr, language = 'french')\n",
    "for sent in sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation de texte en tokens\n",
    "\n",
    "La segmentation peut-être plus fine qu'une segmentation en phrases. Elle peut se faire en tokens. Un token peut correspondre à un mot, un chiffre, une ponctuation ou un symbole. Il est aussi possible de segmenter en n tokens successifs (on parle de n-grammes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la phrase à tokeniser\n",
    "text_fr = \"Les tokens peuvent être des symboles $ ++, des chiffres 7 99, de la ponctuation !? des mots.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage du texte par le pipeline\n",
    "doc_fr = nlp_fr(text_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Les', 'tokens', 'peuvent', 'être', 'des', 'symboles', '$', '+', '+', ',', 'des', 'chiffres', '7', '99', ',', 'de', 'la', 'ponctuation', '!', '?', 'des', 'mots', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation de la phrase et affichage du résultat\n",
    "words = [w.text for w in doc_fr]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction de tokenisation\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Les', 'tokens', 'peuvent', 'être', 'des', 'symboles', '$', '++', ',', 'des', 'chiffres', '7', '99', ',', 'de', 'la', 'ponctuation', '!', '?', 'des', 'mots', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation de la phrase et affichage du résultat\n",
    "words = word_tokenize(text_fr, language = 'french')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Génération de n-grammes avec NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction permettant de générer les n-grammes\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation d'une phrase test\n",
    "words = word_tokenize(\"Le NLP avec NLTK, c'est génial!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Le', 'NLP'), ('NLP', 'avec'), ('avec', 'NLTK'), ('NLTK', ','), (',', \"c'est\"), (\"c'est\", 'génial'), ('génial', '!')]\n"
     ]
    }
   ],
   "source": [
    "# Génération et affichage de bi-grammes à partir de notre phrase test tokenisée \n",
    "bigrams=ngrams(words,2)\n",
    "print(list(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Le', 'NLP', 'avec'), ('NLP', 'avec', 'NLTK'), ('avec', 'NLTK', ','), ('NLTK', ',', \"c'est\"), (',', \"c'est\", 'génial'), (\"c'est\", 'génial', '!')]\n"
     ]
    }
   ],
   "source": [
    "# Génération et affichage de tri-grammes à partir de notre phrase test tokenisée \n",
    "trigrams=ngrams(words,3)\n",
    "print(list(trigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction d'informations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-Of-Speech Tagging\n",
    "\n",
    "Cette tâche consiste à associer un mot à sa classe morphosyntaxique (nom, verbe, adjectif,...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de phrases test, une en français et une en anglais\n",
    "text_fr = \"Je vais au parc avec mon chien\"\n",
    "text_en = \"I go to the park with my dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage des phrases par leur pipeline respectif\n",
    "doc_fr = nlp_fr(text_fr)\n",
    "doc_en = nlp_en(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : Je, , Tag : PRON\n",
      "Word : vais, , Tag : AUX\n",
      "Word : au, , Tag : PRON\n",
      "Word : parc, , Tag : NOUN\n",
      "Word : avec, , Tag : ADP\n",
      "Word : mon, , Tag : DET\n",
      "Word : chien, , Tag : NOUN\n"
     ]
    }
   ],
   "source": [
    "# Affichage de chaque token de la phase française suivi de son tag \n",
    "for token in doc_fr:\n",
    "    print('Word : {0}, , Tag : {1}' .format(token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : I, , Tag : PRP\n",
      "Word : go, , Tag : VBP\n",
      "Word : to, , Tag : IN\n",
      "Word : the, , Tag : DT\n",
      "Word : park, , Tag : NN\n",
      "Word : with, , Tag : IN\n",
      "Word : my, , Tag : PRP$\n",
      "Word : dog, , Tag : NN\n"
     ]
    }
   ],
   "source": [
    "# Affichage de chaque token de la phase anglaise suivi de son tag \n",
    "for token in doc_en:\n",
    "    print('Word : {0}, , Tag : {1}' .format(token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction de taggage\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation de la phrase anglaise\n",
    "tokens_en = word_tokenize(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('go', 'VBP'), ('to', 'TO'), ('the', 'DT'), ('park', 'NN'), ('with', 'IN'), ('my', 'PRP$'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Taggage des tokens et affichage du résultat\n",
    "tags_en = pos_tag(tokens_en)\n",
    "print (tags_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconnaissance d'entités nommées (NER)\n",
    "\n",
    "Cela consiste à reconnaître les mots d’un texte qui correspondent à des concepts catégorisables (noms de personnes, lieux, organisations,...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une phrase test\n",
    "text_en = \"Mark Elliot Zuckerberg (born May 14, 1984) is a co-founder of Facebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage de la phrase test par le pipeline\n",
    "doc_en = nlp_en(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : Mark, , Entity : PERSON\n",
      "Word : Elliot, , Entity : PERSON\n",
      "Word : Zuckerberg, , Entity : PERSON\n",
      "Word : (, , Entity : \n",
      "Word : born, , Entity : \n",
      "Word : May, , Entity : DATE\n",
      "Word : 14, , Entity : DATE\n",
      "Word : ,, , Entity : DATE\n",
      "Word : 1984, , Entity : DATE\n",
      "Word : ), , Entity : \n",
      "Word : is, , Entity : \n",
      "Word : a, , Entity : \n",
      "Word : co, , Entity : \n",
      "Word : -, , Entity : \n",
      "Word : founder, , Entity : \n",
      "Word : of, , Entity : \n",
      "Word : Facebook, , Entity : ORG\n",
      "Word : ., , Entity : \n"
     ]
    }
   ],
   "source": [
    "# Affichage de chaque token et du type d'entité si une entité a été reconnue\n",
    "for token in doc_en:\n",
    "    print('Word : {0}, , Entity : {1}' .format(token.text, token.ent_type_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction de NER\n",
    "from nltk import ne_chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation de la phrase test\n",
    "tokens_en = word_tokenize(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taggage de la phrase tokenisée\n",
    "tags_en = pos_tag(tokens_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  (PERSON Elliot/NNP Zuckerberg/NNP)\n",
      "  (/(\n",
      "  born/VBN\n",
      "  May/NNP\n",
      "  14/CD\n",
      "  ,/,\n",
      "  1984/CD\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  co-founder/NN\n",
      "  of/IN\n",
      "  (GPE Facebook/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Application de la fonction de NER sur les tokens taggés et affichage du résultat\n",
    "ner_en = ne_chunk(tags_en)\n",
    "print (ner_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
