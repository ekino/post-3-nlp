{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook accompagne le post : \"Handson de quelques tâches courantes en NLP\" (ou sa version anglaise \"Simple NLP tasks tutorial\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement des modèles SpaCy via le notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en # modèle anglais\n",
    "!python -m spacy download fr # modèle français"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des librairies SpaCy et NLTK et création des \"language processing pipelines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load(\"fr\")\n",
    "nlp_en = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation de texte en phrases\n",
    "\n",
    "Il s'agit de repérer le début et la fin de chaque phrase d'un texte pour pouvoir la segmenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du texte à segmenter\n",
    "text_fr = \"Ceci est 1 première phrase. Puis j'en écris une seconde. pour finir en voilà une troisième sans mettre de majuscule\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage du texte par le pipeline\n",
    "doc_fr = nlp_fr(text_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation du texte et affichage du résulat\n",
    "for sent in doc_fr.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction permettant la segmentation en phrases\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation du texte et affichage du résultat\n",
    "sentences = sent_tokenize(text_fr, language = 'french')\n",
    "for sent in sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation de texte en tokens\n",
    "\n",
    "La segmentation peut-être plus fine qu'une segmentation en phrases. Elle peut se faire en tokens. Un token peut correspondre à un mot, un chiffre, une ponctuation ou un symbole. Il est aussi possible de segmenter en n tokens successifs (on parle de n-grammes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la phrase à tokeniser\n",
    "text_fr = \"Les tokens peuvent être des symboles $ ++, des chiffres 7 99, de la ponctuation !? des mots.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage du texte par le pipeline\n",
    "doc_fr = nlp_fr(text_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation de la phrase et affichage du résultat\n",
    "words = [w.text for w in doc_fr]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction de tokenisation\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation de la phrase et affichage du résultat\n",
    "words = word_tokenize(text_fr, language = 'french')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Génération de n-grammes avec NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction permettant de générer les n-grammes\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation d'une phrase test\n",
    "words = word_tokenize(\"Le NLP avec NLTK, c'est génial!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération et affichage de bi-grammes à partir de notre phrase test tokenisée \n",
    "bigrams=ngrams(words,2)\n",
    "print(list(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération et affichage de tri-grammes à partir de notre phrase test tokenisée \n",
    "trigrams=ngrams(words,3)\n",
    "print(list(trigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction d'informations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-Of-Speech Tagging\n",
    "\n",
    "Cette tâche consiste à associer un mot à sa classe morphosyntaxique (nom, verbe, adjectif,...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de phrases test, une en français et une en anglais\n",
    "text_fr = \"Je vais au parc avec mon chien\"\n",
    "text_en = \"I go to the park with my dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage des phrases par leur pipeline respectif\n",
    "doc_fr = nlp_fr(text_fr)\n",
    "doc_en = nlp_en(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de chaque token de la phase française suivi de son tag \n",
    "for token in doc_fr:\n",
    "    print('Word : {0}, , Tag : {1}' .format(token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de chaque token de la phase anglaise suivi de son tag \n",
    "for token in doc_en:\n",
    "    print('Word : {0}, , Tag : {1}' .format(token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction de taggage\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation de la phrase anglaise\n",
    "tokens_en = word_tokenize(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taggage des tokens et affichage du résultat\n",
    "tags_en = pos_tag(tokens_en)\n",
    "print (tags_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconnaissance d'entités nommées (NER)\n",
    "\n",
    "Cela consiste à reconnaître les mots d’un texte qui correspondent à des concepts catégorisables (noms de personnes, lieux, organisations,...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une phrase test\n",
    "text_en = \"Mark Elliot Zuckerberg (born May 14, 1984) is a co-founder of Facebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage de la phrase test par le pipeline\n",
    "doc_en = nlp_en(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de chaque token et du type d'entité si une entité a été reconnue\n",
    "for token in doc_en:\n",
    "    print('Word : {0}, , Entity : {1}' .format(token.text, token.ent_type_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction de NER\n",
    "from nltk import ne_chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation de la phrase test\n",
    "tokens_en = word_tokenize(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taggage de la phrase tokenisée\n",
    "tags_en = pos_tag(tokens_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de la fonction de NER sur les tokens taggés et affichage du résultat\n",
    "ner_en = ne_chunk(tags_en)\n",
    "print (ner_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
